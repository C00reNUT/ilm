@article{see2019massively,
  title={Do Massively Pretrained Language Models Make Better Storytellers?},
  author={See, Abigail and Pappu, Aneesh and Saxena, Rohun and Yerukola, Akhila and Manning, Christopher D},
  journal={arXiv:1909.10705},
  year={2019}
}

@inproceedings{zellers2019defending,
  title={Defending against neural fake news},
  author={Zellers, Rowan and Holtzman, Ari and Rashkin, Hannah and Bisk, Yonatan and Farhadi, Ali and Roesner, Franziska and Choi, Yejin},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NeurIPS},
  year={2017}
}

@article{shen2020blank,
  title={Blank Language Models},
  author={Shen, Tianxiao and Quach, Victor and Barzilay, Regina and Jaakkola, Tommi},
  journal={arXiv:2002.03079},
  year={2020}
}

@inproceedings{papineni2002bleu,
  title={{BLEU}: A method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={ACL},
  year={2002},
}

@article{assael2019restoring,
  title={Restoring ancient text using deep learning: a case study on Greek epigraphy},
  author={Assael, Yannis and Sommerschield, Thea and Prag, Jonathan},
  journal={arXiv:1910.06262},
  year={2019}
}

@article{sennrich2015neural,
  title={Neural machine translation of rare words with subword units},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  journal={arXiv preprint arXiv:1508.07909},
  year={2015}
}